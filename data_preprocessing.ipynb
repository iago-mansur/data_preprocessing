{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import zscore, skew\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, QuantileTransformer, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and editing the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.reaf_csv('data.csv')\n",
    "\n",
    "df_1 = database.iloc[:,:].values\n",
    "\n",
    "df_merged = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "dataset = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.attrs = {\n",
    "    'col1': ' Description of col1',\n",
    "    'col2': ' Description of col2'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_excel('file1.xlsx', sheet_name=None)\n",
    "df_1['col'].columns = df_1['col'].iloc[5]\n",
    "df_1['col'] = df_1['col'].iloc[6:,:]\n",
    "df_1_all = pd.concat(df_1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_excel('file2.xlsx', sheet_name=None)\n",
    "df_2_all = pd.concat(df_2, ignore_index=True)\n",
    "dataset = pd.concat([df_1_all,df_2_all], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reset_index(inplace=True, drop=True)\n",
    "dataset.drop(dataset.columns[[2, 4, 6, 8, 10, 12]], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Checking the data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Changing data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['col1'] = pd.to_datetime(dataset['col1'], errors='coerce')\n",
    "dataset['col2'] = pd.to_numeric(dataset['col2'], errors='coerce')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop_duplicates(subset=\"col_name\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Removing irrelevant observations and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.loc[dataset['col_name'] == 'specific_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Removing unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns=['unnecessary_col_1', 'unnecessary_col_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Handling inconsistent data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Deleting Rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Impute missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#       Method 1\n",
    "for col in ['col_1', 'col_2']:\n",
    "        dataset[col] = round(dataset[col].fillna(dataset[col].median()))\n",
    "\n",
    "#       Method 2\n",
    "imputer = SimpleImputer(minssing_valeues = np.nan, strategy='mean')\n",
    "imputer.fit(dataset[:,:])\n",
    "imputer.transform(database[:, :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection\n",
    "    Selecting the relevant variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.compose import ColumnTransformer\n",
    "# ct = ColumnTransformer(transformers=[('enconder', OneHotEncoder, ['col_name'])], remainder='passthrough')\n",
    "# dataset = np.array(ct.fit_transform(dataset))\n",
    "\n",
    "ohe = OneHotEncoder(\n",
    "        categories='auto',  # Categories per feature\n",
    "        drop=None, # Whether to drop one of the features\n",
    "        sparse=True, # Will return sparse matrix if set True\n",
    "        dtype=<class 'numpy.float64'>, # Desired data type of the output\n",
    "        handle_unknown='error' # Whether to raise an error \n",
    "    )\n",
    "\n",
    "dataset = ohe.fit_transform(dataset['col_name'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Detecting outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Boxplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsize_boxplot = (1, 2)\n",
    "\n",
    "fig_boxplot = plt.figure(figsize=(12, 8))\n",
    "\n",
    "boxplot1 = plt.subplot2grid(gridsize_boxplot, (0, 0))\n",
    "boxplot2 = plt.subplot2grid(gridsize_boxplot, (0, 1))\n",
    "\n",
    "boxplot1.set_title('Title 1', fontsize=8)\n",
    "boxplot1.boxplot(dataset['col1'])\n",
    "\n",
    "boxplot2.set_title('Title 2', fontsize=8)\n",
    "boxplot2.boxplot(dataset['col2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsize_scatter = (1, 2)\n",
    "\n",
    "fig_scatter = plt.figure(figsize=(12, 8))\n",
    "\n",
    "scatter1 = plt.subplot2grid(gridsize_scatter, (0, 0))\n",
    "scatter2 = plt.subplot2grid(gridsize_scatter, (0, 1))\n",
    "\n",
    "scatter1.set_title('Title 1', fontsize=8)\n",
    "scatter1.scatter(dataset['col1'])\n",
    "\n",
    "scatter2.set_title('Title 2', fontsize=8)\n",
    "scatter2.scatter(dataset['col2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsize_hist = (1, 2)\n",
    "\n",
    "fig_hist = plt.figure(figsize=(12, 8))\n",
    "\n",
    "hist1 = plt.subplot2grid(gridsize_hist, (0, 0))\n",
    "hist2 = plt.subplot2grid(gridsize_hist, (0, 1))\n",
    "\n",
    "hist1.set_title('Title 1', fontsize=8)\n",
    "hist1.hist(dataset['col1'])\n",
    "\n",
    "hist2.set_title('Title 2', fontsize=8)\n",
    "hist2.hist(dataset['col2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Parametric or Non-parametric test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Removing / Transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            Standardization/Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(dataset.columns)\n",
    "cols.remove('col')\n",
    "dataset[cols] = dataset[cols].apply(zscore)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"3 - means that 99.7% of the data is saved\n",
    "2 - means that 95.44% of the data is saved\n",
    "1 - means that 68.26% of the data is saved\"\"\"\n",
    "\n",
    "outliers_threshold = 1\n",
    "\n",
    "selected_dada = dataset[cols].abs() <= outliers_threshold\n",
    "\n",
    "not_selected_dada = ~(selected_dada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (selected_dada).all(axis=1)\n",
    "dataset = dataset[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(dataset.columns)\n",
    "cols.remove('col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# calculate max and min sacaling\n",
    "scaler.fit(dataset[cols])\n",
    "\n",
    "# scale all data points using max and min sacaling\n",
    "scaled_features = scaler.transform(dataset[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(dataset.columns)\n",
    "cols.remove('col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# calculate median and IQR\n",
    "robust_scaler.fit(dataset[cols])\n",
    "\n",
    "# scale all data points using median and IQR\n",
    "robust_scaled_data = robust_scaler.transform(dataset[cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Feature Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            Winsorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(dataset.columns)\n",
    "cols.remove('col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winsorization_flooring = 0.05\n",
    "winsorization_capping = 1 - winsorization_flooring\n",
    "\n",
    "not_selected_data_flooring = dataset[cols] <= dataset[cols].quantile(winsorization_flooring)\n",
    "not_selected_data_capping = dataset[cols] >= dataset[cols].quantile(winsorization_capping)\n",
    "\n",
    "not_selected_data_all = (not_selected_data_flooring & not_selected_data_capping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = dataset[cols][not_selected_data_all].index\n",
    "dataset[cols].drop(data_index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            Clipping - Arbitrary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1_clipping_flooring = ?\n",
    "col1_clipping_capping = ?\n",
    "\n",
    "col2_clipping_flooring = ?\n",
    "col2_clipping_capping = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Removing target values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Changing the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.replace(old_value, new_value, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Detecting skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dataset['col'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The acceptable value of skewness is considered to be between -3 and +3.\n",
    "#If the value of skewness falls on either side of this range then it indicates that the skewness must be fixed.\n",
    "\n",
    "print(skew(dataset, axis=0, bias=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Data transformations to manage skewness: logarithmic transformation (positive values only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_data = np.log(dataset['col'])\n",
    "\n",
    "plt.hist(log_data, bins=50, label='Log-Transformed Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Dealing with negative values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = QuantileTransformer(n_quantiles=10, random_state=0)\n",
    "qt.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collinearity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Identifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplots\n",
    "sns.pairplot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation coefficient\n",
    "# The range for a correlation coefficient is between -1 and 1\n",
    "# Typically, we say that two variables are correlated if \n",
    "# their correlation coefficient is greater than 0.5, or less than -0.5.\n",
    "corr = X.corr()\n",
    "print(\"Correlation Coefficients\")\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Spearman's Rank correlation coefficient\n",
    "# Measures the monotonic relation between a pair of variables\n",
    "# their correlation coefficient is greater than 0.5, or less than -0.5.\n",
    "\n",
    "corr = X.corr(method=\"spearman\")\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Inflation Factor(VIF)\n",
    "# Generally, a VIF above 5 indicates a high multicollinearity.\n",
    "# VIF between 1 and 5: There is moderate correlation between a given predictor variable and other predictor variables in the model.\n",
    " \n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) \n",
    "                          for i in range(len(X.columns))] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Removing dependent (highly correlated) variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Transforming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2) # Increase to fit on final model.\n",
    "dataset_reduced = pca.fit_transform(dataset)\n",
    "\n",
    "# Visualizing importance of the componentes\n",
    "pca = PCA().fit(dataset)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "\n",
    "pca = PCA(0.50).fit(dataset)\n",
    "pca.n_components_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
